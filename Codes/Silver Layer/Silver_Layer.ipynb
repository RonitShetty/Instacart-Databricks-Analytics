{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56c4973-415d-41b1-9c36-e0a6dcda4174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing silver_orders...\nCreated silver_orders\nProcessing silver_products...\nCreated silver_products. Rows: 49687\nProcessing silver_order_items...\nCreated silver_order_items\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, lit\n",
    "\n",
    "spark.sql(\"USE CATALOG workspace\")\n",
    "spark.sql(\"USE SCHEMA instacart\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. CREATE SILVER_ORDERS\n",
    "# ==========================================\n",
    "print(\"Processing silver_orders...\")\n",
    "df_orders = spark.read.table(\"bronze_orders\")\n",
    "\n",
    "# Replace nulls in 'days_since_prior_order' with 0\n",
    "df_orders_clean = df_orders.fillna(0, subset=[\"days_since_prior_order\"])\n",
    "\n",
    "df_orders_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_orders\")\n",
    "print(\"Created silver_orders\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. CREATE SILVER_PRODUCTS (Fixed with expr)\n",
    "# ==========================================\n",
    "print(\"Processing silver_products...\")\n",
    "df_products = spark.read.table(\"bronze_products\")\n",
    "df_aisles = spark.read.table(\"bronze_aisles\")\n",
    "df_departments = spark.read.table(\"bronze_departments\")\n",
    "\n",
    "# FIX: Use expr() to access the SQL function try_cast\n",
    "# This converts \" Blunted\" -> NULL safely without crashing\n",
    "df_products_clean = df_products \\\n",
    "    .withColumn(\"aisle_id\", expr(\"try_cast(aisle_id as int)\")) \\\n",
    "    .withColumn(\"department_id\", expr(\"try_cast(department_id as int)\")) \\\n",
    "    .filter(col(\"aisle_id\").isNotNull() & col(\"department_id\").isNotNull()) \n",
    "\n",
    "# Join with lookup tables\n",
    "df_products_joined = df_products_clean \\\n",
    "    .join(df_aisles, \"aisle_id\", \"left\") \\\n",
    "    .join(df_departments, \"department_id\", \"left\") \\\n",
    "    .select(\n",
    "        col(\"product_id\"), \n",
    "        col(\"product_name\"), \n",
    "        col(\"aisle\"), \n",
    "        col(\"department\"), \n",
    "        col(\"aisle_id\"), \n",
    "        col(\"department_id\")\n",
    "    )\n",
    "\n",
    "df_products_joined.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_products\")\n",
    "print(f\"Created silver_products. Rows: {df_products_joined.count()}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. CREATE SILVER_ORDER_ITEMS\n",
    "# ==========================================\n",
    "print(\"Processing silver_order_items...\")\n",
    "df_prior = spark.read.table(\"bronze_order_products__prior\")\n",
    "df_train = spark.read.table(\"bronze_order_products__train\")\n",
    "\n",
    "df_order_items = df_prior.union(df_train)\n",
    "\n",
    "df_order_items.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_order_items\")\n",
    "print(\"Created silver_order_items\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}